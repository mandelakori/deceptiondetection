This paper explores the evolving landscape of digital deception and presents foundational techniques for text-based and multimodal fraud detection. It begins with an analysis of AI-enabled fraud trends, such as deepfakes and generative AI scams, and shifts from rule-based systems to behavioral analysis. The static text-based approach covers advanced preprocessing (e.g., text normalization, leetspeak handling, and sensitive information masking), feature engineering (e.g., TF-IDF, linguistic features), and modeling paradigms (e.g., machine learning with XGBoost, deep learning with transformers, and hybrid models). Performance benchmarks are provided, highlighting trade-offs in accuracy, interpretability, and latency.
The multimodal section discusses integrating audio and text signals to address the "modality gap," including deep audio feature extraction (e.g., MFCCs, prosodic markers, wav2vec embeddings) and fusion strategies (early, late, and mid-fusion with cross-attention). It addresses data challenges through synthetic generation and MLOps practices for production deployment, such as experiment tracking, drift mitigation, and real-world case studies demonstrating fraud reduction and improved efficiency.
The conclusion emphasizes hybrid architectures for adaptability and future directions, including Large Audio Language Models (LALMs) and adversarial data synthesis. Comparative tables summarize static vs. multimodal approaches, making this a valuable resource for researchers and practitioners in AI-driven security.
